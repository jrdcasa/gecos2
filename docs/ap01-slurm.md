Brief guide to install SLURM in an UBUNTU
==============================================================

[SLURM](https://slurm.schedmd.com/documentation.html) is a scheduling tool widely used in HPC clusters. In addition, it can be an valuable tool on a local or single node. This tool allow you to run several progrmas at once and queue them to schedule its execution. 
The current document show you how to quickly install and setup [SLURM](https://slurm.schedmd.com/documentation.html) on a single machine with Ubuntu 20.04. For other linux distributions, you can find information on internet. 

The packages needed are ``slurmctld`` and ``slurmd`` for a basic single machine setup. These can be installed using the ``apt`` installation tool available in ubuntu:

```
$ sudo apt update -y
$ sudo apt install slurmd slurmctld -y
```

Next step, we need to create the configuration file ``slurm.conf``. We can create this file using the configration tool in the [slurm web page](https://slurm.schedmd.com/configurator.html)

```
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ClusterName=clusterlocal
SlurmctldHost=localhost
#SlurmctldHost=
#
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=67043328
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=lua
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=10000
#MaxStepCount=40000
#MaxTasksPerNode=512
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/linuxproc
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
#
#
# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/basic
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
#AccountingStorageHost=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/filetxt
#AccountingStorageUser=
#AccountingStoreFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#DebugFlags=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
NodeName=ubuntu2004 CPUs=2 RealMemory=3000 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 State=UNKNOWN
PartitionName=cpu Nodes=ALL Default=YES MaxTime=INFINITE State=UP

```

In NodeName must appear the name of the node give by ``hostname``

Copy this file to /etc/slurm-llnl

```
$ sudo cp slurm.conf /etc/slurm-llnl
$ sudo chmod 755 /etc/slurm-llnl/slurm.conf
$ sudo touch /var/log/slurm_jobacct.log
$ sudo chown slurm:slurm /var/log/slurm_jobacct.log
```

Now it is time to start the services

```
$ sudo systemctl start slurmctld
$ sudo systemctl start slurmd

$ sudo systemctl status slurmctld
$ sudo systemctl status slurmd
```

To check if the slurm system is running, you will see something like the above:

```
ubuntu@ubuntu2004:~$ sudo scontrol update nodename=ubuntu2004 state=idle
ubuntu@ubuntu2004:~$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
cpu*         up   infinite      1  drain ubuntu2004
```

In this case, there is a partition called cpu. However, there is a problem that drained the node, to kown the reason write:
```
ubuntu@ubuntu2004:~/gecos$ sinfo -R
REASON               USER      TIMESTAMP           NODELIST
Low RealMemory       slurm     2022-03-17T10:32:28 ubuntu2004
```
 You need to make changes in the ``slurm.conf`` file and restart both services ``systemctld`` and ``systemd`` services. In this example, change the value of RealMemory in the NodeName section of the ``slurm.conf´´. In addition, you can find information of any issues in the following log files:

```
sudo cat /var/log/slurm-llnl/slurmd.log 
sudo cat /var/log/slurm-llnl/slurmctld.log
```
After make the changes, 
```
ubuntu@ubuntu2004:~/gecos$ sinfo 
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
cpu*         up   infinite      1   idle ubuntu2004
```

Now the slurm system is working.


